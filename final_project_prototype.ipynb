{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Art Pieces Based on Style\n",
    "\n",
    "## Description of the project\n",
    "\n",
    "The goal of this project is to use deep learning in order to classify different art pieces based on style. I will use a TensorFlow's implementation of a CNN, as convolutional neural networks tend to produce good results when it comes to image classification in general. Keep in mind that many parts of the project are not done yet. Also, note that, in order to run the code provided here and the code that will come with the rest of the project, one will  naturally need to install TensorFlow on their system. The installation through *pip* was pretty simple and I just followed the instructions given by the book.\n",
    "\n",
    "The dataset that I will be using for the project can be found on Kaggle, and will, of course, be included in the submission folder. The dataset is split into 2 parts: \"dataset\" & \"museum_art\", with \"dataset\" being larger and more comprehensive. For the project itself, I think that I will have more than enough images in the \"dataset\" directory, but if I end up thinking that there could be benefits to using both of those, that part shouldn't be too hard to change in the code later on in the project. Again, I think that there is enough material in the 'dataset' directory for the CNN to achieve fairly good results. Furthermore, both of those directories are split into 'training' and 'validation' sets, which is a nice feature provided by the person who created this dataset on Kaggle. I think that I will stick to that same split, but again, I am leaving myself some freedom here. Both the training and the validation portions are then split again, into directories corresponding to different classes: 'drawing', 'engraving', 'iconography', 'painting', 'sculpture'.\n",
    "\n",
    "The first part of the project is definitely data processing. All of the image files need to be labeled correctly and then preprocessed using the features given to us by TensorFlow, specifically tensorflow.io and tensorflow.image modules.\n",
    "\n",
    "Then comes the actual learning. Given what I have read online and in the book, there is a high chance that the algorithm will not perform well on the first try. Convolutonal neural networks, and deep neural networks in general, tend to overfit the data. Now since I am not planning on making an ensemble of multiple CNNs (that would be too complicated and computationally-intensive for what I'm trying to do), I will stick to the good old dropout to try to regularize the CNN a bit and improve its generalization property. This is an area for which I do have to do a bit more research, so particular ideas implementation-wise may change in these next few weeks.\n",
    "\n",
    "The last part of the whole process will be testing and evaluation. In my Project Proposal assignment, I said that I was going to do k-fold cross validation for this. Now that I've done a lot of research about CNN's I think that this is unrealistic and impractical. As far I can tell from researching things online, k-fold cross validation is not often used with CNN's because there are simply too many parameters to look at. As far as the other, individual metrics, go, I think that I will mostly be concerned with accuracy. I put in my Project Proposal assignment that I would look at true positive and false negative rates, but even if I do that, it will definitely not be the focus of the project. Most of my time until the end of the semester will be spent in trying to make the network perform as best as it can, accuracy-wise.\n",
    "\n",
    "## Status\n",
    "\n",
    "It turns out that I was correct when I was initially writing this little introduction. A decent portion of the first part is done. I was able to familiarize myself with file paths in Python and use them nicely to properly name all the image files. I was also able to remove all the corrupted images that were initially making a lot of trouble when being provided as input to *tf.io.read_file* and *tf.image.decode_image*, which are the main two methods that I needed to use to process images using TensorFlow. The main question that remains to be answered for this part is how to effectively split the training data into baches that I will feed into the CNN itself. There are examples of this in the book and online and I am quite confident that it will not be tremendously hard to do. With that part done, this portion of the assignment should be done.\n",
    "\n",
    "As far as other portions of the work go, the second and third part of the assignment are still remaining. I am hopeful that at the end of the following week, I will be pretty much done with the remaining parts of the data processing portion and with the learning portion, so that I can start brainstorming ideas that I could use to optimize this whole process further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing corrupt data and renaming files\n",
    "\n",
    "Note that when I downloaded the dataset, all the files were named in a weird way and there was no convention present. I decided to name all the files in the following way. Each file will contain essentially three pieces of information. The first piece will tell us whether this file (image) is part of the training dataset or the testing dataset. The next part will tell us which class this particular file belongs to. The final part of the name will be the number of the image in the corresponding class.\n",
    "\n",
    "And so an example of this naming convention would be:\n",
    "\n",
    "_**training-drawing-123**_\n",
    "\n",
    "This file would, of course, correspond to the 123rd .jpg file present in the training portion of the drawing class. Note that I am not including the extension of the file, as all of them are either going to be .jpg or .jpeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that because of the image verification, it takes a while to run this code.\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import string\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# Forming the necessary paths\n",
    "dataset_dir = pathlib.Path('.') / 'Data' / 'dataset'\n",
    "museum_art_dir = pathlib.Path('.') / 'Data' / 'museum_art'\n",
    "\n",
    "# Renaming individual files while traversing the file tree and removing any corrupt images\n",
    "for directory in [dataset_dir, museum_art_dir]:\n",
    "    for type_of_data in directory.iterdir():\n",
    "        for class_label in type_of_data.iterdir():\n",
    "            current_dir = class_label\n",
    "            for index, image_file in enumerate(current_dir.iterdir()):\n",
    "                try:\n",
    "                    v_image = Image.open(image_file)\n",
    "                    v_image.verify()\n",
    "                    # If we pass these two checks means the image is not corrupt\n",
    "                    old_file = current_dir / image_file.name\n",
    "                    new_name = '{}-{}-{}'.format(type_of_data.name, class_label.name, index)\n",
    "                    new_name += image_file.suffix\n",
    "                    new_file = current_dir / new_name\n",
    "                    # We also want to check whether we already created the given file\n",
    "                    try:\n",
    "                        os.rename(old_file, new_file)\n",
    "                    except IOError as e:\n",
    "                        pass\n",
    "                except PIL.UnidentifiedImageError:\n",
    "                    os.remove(image_file)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing some representative images\n",
    "\n",
    "I will use matplotlib to plot a few of the images from each class, just so that we can see what we're dealing with and so that I can practice using the TensorFlow API to process images. When I start using the Keras API, the specifics of processing images could change, but the important thing is that, even if I don't find a better way to do this, this one seems to be working just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "# Get the correct path\n",
    "training_data = pathlib.Path('.') / 'Data' / 'dataset' / 'training'\n",
    "drawing_path = training_data / 'drawing'\n",
    "engraving_path = training_data / 'engraving'\n",
    "iconography_path = training_data / 'iconography'\n",
    "painting_path = training_data / 'painting'\n",
    "sculpture_path = training_data / 'sculpture'\n",
    "\n",
    "# Creating the corresponding lists\n",
    "drawing_list = []\n",
    "engraving_list = []\n",
    "iconography_list = []\n",
    "painting_list = []\n",
    "sculpture_list = []\n",
    "\n",
    "path_list = [drawing_path, engraving_path, iconography_path, painting_path, sculpture_path]\n",
    "\n",
    "# Populate all the lists at once\n",
    "for path in path_list:\n",
    "    for index, image_file in enumerate(path.iterdir(), 1):\n",
    "        if index > 10:\n",
    "            break\n",
    "        if path == path_list[0]:\n",
    "            # Add to the list of drawings\n",
    "            drawing_list.append(image_file)\n",
    "        elif path == path_list[1]:\n",
    "            # Add to the list of engravings\n",
    "            engraving_list.append(image_file)\n",
    "        elif path == path_list[2]:\n",
    "            # Add to the list for iconography\n",
    "            iconography_list.append(image_file)\n",
    "        elif path == path_list[3]:\n",
    "            # Add to the list of paintings\n",
    "            painting_list.append(image_file)\n",
    "        else:\n",
    "            # Add to the list of sculptures\n",
    "            sculpture_list.append(image_file)\n",
    "            \n",
    "# Define a function for plotting\n",
    "def plot_images(image_list):\n",
    "    figure = plt.figure(figsize=(20, 10))\n",
    "    subplot_list = []\n",
    "    for index, image_file in enumerate(image_list):\n",
    "        # Have to use os.path.real_path here because\n",
    "        # tf is complaining about pathlib.Path\n",
    "        raw_image = tf.io.read_file(os.path.realpath(image_file))\n",
    "        image = tf.image.decode_image(raw_image)\n",
    "        ax = figure.add_subplot(2, 5, index+1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(image_file.name, size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test out the function with the list of drawings\n",
    "# plot_images(drawing_list)\n",
    "\n",
    "# Same function can be used to show other images as well:\n",
    "# plot_images(engraving_list)\n",
    "# plot_images(iconography_list)\n",
    "# plot_images(painting_list)\n",
    "# plot_images(sculpture_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing data and connecting images to class labels\n",
    "\n",
    "In this part, I will be trying to figure out a way to handle the size of my dataset, which is not small by any means. I will have to somehow split the training data into batches and then create tensors from those batches. Furthermore, I will have to construct tensors (or possibly even batches) of class labels for each tensor (or batch) of data that I'm using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for constructing TF Datasets from existing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.2, shape=(), dtype=float32)\n",
      "tf.Tensor(3.4, shape=(), dtype=float32)\n",
      "tf.Tensor(7.5, shape=(), dtype=float32)\n",
      "tf.Tensor(4.1, shape=(), dtype=float32)\n",
      "tf.Tensor(5.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = [1.2, 3.4, 7.5, 4.1, 5.0, 1.0]\n",
    "# Constructing the dataset\n",
    "ds = tf.data.Dataset.from_tensor_slices(a)\n",
    "\n",
    "# Printing some values\n",
    "for item in ds:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for constructing batches from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: [1.2 3.4 7.5]\n",
      "batch 2: [4.1 5.  1. ]\n"
     ]
    }
   ],
   "source": [
    "# Constructing batches of size 3\n",
    "ds_batch = ds.batch(3)\n",
    "\n",
    "# Printing some values\n",
    "for i, elem in enumerate(ds_batch, 1):\n",
    "    print('batch {}:'.format(i), elem.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for combining two tensors into a joint dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.16513085 0.9014813  0.6309742 ]\n",
      " [0.4345461  0.29193902 0.64250207]\n",
      " [0.9757855  0.43509948 0.6601019 ]\n",
      " [0.60489583 0.6366315  0.6144488 ]], shape=(4, 3), dtype=float32)\n",
      "tf.Tensor([0 1 2 3], shape=(4,), dtype=int32)\n",
      " x: [0.16513085 0.9014813  0.6309742 ]  y: 0\n",
      " x: [0.4345461  0.29193902 0.64250207]  y: 1\n",
      " x: [0.9757855  0.43509948 0.6601019 ]  y: 2\n",
      " x: [0.60489583 0.6366315  0.6144488 ]  y: 3\n",
      " x: [0.16513085 0.9014813  0.6309742 ]  y: 0\n",
      " x: [0.4345461  0.29193902 0.64250207]  y: 1\n",
      " x: [0.9757855  0.43509948 0.6601019 ]  y: 2\n",
      " x: [0.60489583 0.6366315  0.6144488 ]  y: 3\n"
     ]
    }
   ],
   "source": [
    "# Creating two tensors\n",
    "tf.random.set_seed(1)\n",
    "t_x = tf.random.uniform([4, 3], dtype=tf.float32)\n",
    "t_y = tf.range(4)\n",
    "\n",
    "# Printing the two tensors\n",
    "print(t_x)\n",
    "print(t_y)\n",
    "\n",
    "# Creating two datasets and then zipping them\n",
    "ds_x = tf.data.Dataset.from_tensor_slices(t_x)\n",
    "ds_y = tf.data.Dataset.from_tensor_slices(t_y)\n",
    "ds_joint_1 = tf.data.Dataset.zip((ds_x, ds_y))\n",
    "\n",
    "# Printing some output\n",
    "for example in ds_joint_1:\n",
    "    print(' x:', example[0].numpy(),\n",
    "          ' y:', example[1].numpy())\n",
    "\n",
    "# Creating one, joint dataset\n",
    "ds_joint_2 = tf.data.Dataset.from_tensor_slices((t_x, t_y))\n",
    "for example in ds_joint_2:\n",
    "    print(' x:', example[0].numpy(),\n",
    "          ' y:', example[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for batching, shuffling and repeating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16513085 0.9014813  0.6309742 ]\n",
      " [0.4345461  0.29193902 0.64250207]\n",
      " [0.9757855  0.43509948 0.6601019 ]]\n",
      "[0 1 2]\n",
      "\n",
      "[[0.60489583 0.6366315  0.6144488 ]]\n",
      "[3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a shuffled version of the ds_joint dataset\n",
    "tf.random.set_seed(1)\n",
    "ds = ds_joint_1.shuffle(buffer_size=len(t_x))\n",
    "# for example in ds:\n",
    "#     print(' x:', example[0].numpy(),\n",
    "#           ' y:', example[1].numpy())\n",
    "\n",
    "# Experimenting with batch function\n",
    "ds = ds_joint_1.batch(batch_size=3,\n",
    "                      drop_remainder=False)\n",
    "for element in ds:\n",
    "    x_part, y_part = element[0], element[1]\n",
    "    print(x_part.numpy())\n",
    "    print(y_part.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (2, 3) [2 1]\n",
      "1 (2, 3) [0 3]\n",
      "2 (2, 3) [0 3]\n",
      "3 (2, 3) [1 2]\n",
      "4 (2, 3) [3 0]\n",
      "5 (2, 3) [1 2]\n",
      "\n",
      "\n",
      "\n",
      "0 (2, 3) [0 1]\n",
      "1 (2, 3) [2 3]\n",
      "2 (2, 3) [0 1]\n",
      "3 (2, 3) [2 3]\n",
      "4 (2, 3) [2 3]\n",
      "5 (2, 3) [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Order 1: shuffle -> batch -> repeat\n",
    "tf.random.set_seed(1)\n",
    "ds = ds_joint_1.shuffle(4).batch(2).repeat(3)\n",
    "for i, (batch_x, batch_y) in enumerate(ds):\n",
    "    print(i, batch_x.shape, batch_y.numpy())\n",
    "print('\\n\\n')\n",
    "\n",
    "# Order 2: batch -> shuffle -> repeat\n",
    "tf.random.set_seed(1)\n",
    "ds = ds_joint_1.batch(2).shuffle(4).repeat(3)\n",
    "for i, (batch_x, batch_y) in enumerate(ds):\n",
    "    print(i, batch_x.shape, batch_y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Training and Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Get the correct paths\n",
    "training_data = pathlib.Path('.') / 'Data' / 'dataset' / 'training'\n",
    "validation_data = pathlib.Path('.') / 'Data' / 'dataset' / 'validation'\n",
    "\n",
    "# Creating lists of training classes and validation classes\n",
    "training_paths = []\n",
    "validation_paths = []\n",
    "for (train_path, valid_path) in zip(training_data.iterdir(), validation_data.iterdir()):\n",
    "    training_paths.append(train_path)\n",
    "    validation_paths.append(valid_path)\n",
    "\n",
    "def get_label_from_path(path):\n",
    "    if 'drawing' in str(path):\n",
    "        return 0\n",
    "    if 'engraving' in str(path):\n",
    "        return 1\n",
    "    if 'iconography' in str(path):\n",
    "        return 2\n",
    "    if 'painting' in str(path):\n",
    "        return 3\n",
    "    if 'sculpture' in str(path):\n",
    "        return 4\n",
    "    return None\n",
    "\n",
    "def load_image_from_path(path, img_width=32, img_height=32):\n",
    "    image = tf.io.read_file(str(path))\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [img_height, img_width])\n",
    "    image /= 255.0\n",
    "    return image\n",
    "\n",
    "training_images_list = []\n",
    "training_labels_list = []\n",
    "for directory in training_paths:\n",
    "    for file_path in directory.iterdir():\n",
    "        training_images_list.append(load_image_from_path(file_path))\n",
    "        training_labels_list.append(get_label_from_path(file_path))\n",
    "\n",
    "# Construct the training dataset\n",
    "training_images_ds = tf.data.Dataset.from_tensor_slices(training_images_list)\n",
    "training_labels_ds = tf.data.Dataset.from_tensor_slices(training_labels_list)\n",
    "\n",
    "validation_images_list = []\n",
    "validation_labels_list = []\n",
    "for directory in validation_paths:\n",
    "    for file_path in directory.iterdir():\n",
    "        validation_images_list.append(load_image_from_path(file_path))\n",
    "        validation_labels_list.append(get_label_from_path(file_path))\n",
    "        \n",
    "# Construct the validation dataset\n",
    "validation_images_ds = tf.data.Dataset.from_tensor_slices(validation_images_list)\n",
    "validation_labels_ds = tf.data.Dataset.from_tensor_slices(validation_labels_list)\n",
    "\n",
    "# Shuffling and batching the data\n",
    "# Shuffle the training images and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_list = tf.random.shuffle(training_images_list, seed=8)\n",
    "training_labels_list = tf.random.shuffle(training_labels_list, seed=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to train a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 122,245\n",
      "Trainable params: 122,245\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pathlib\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(5))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "242/242 [==============================] - 4s 18ms/step - loss: 0.9784 - accuracy: 0.6240 - val_loss: 0.7784 - val_accuracy: 0.7208\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 4s 18ms/step - loss: 0.7372 - accuracy: 0.7371 - val_loss: 0.6800 - val_accuracy: 0.7722\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 4s 18ms/step - loss: 0.6398 - accuracy: 0.7705 - val_loss: 0.6403 - val_accuracy: 0.7629\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 6s 25ms/step - loss: 0.5694 - accuracy: 0.7903 - val_loss: 0.6213 - val_accuracy: 0.7804\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 6s 25ms/step - loss: 0.5180 - accuracy: 0.8081 - val_loss: 0.5244 - val_accuracy: 0.8096\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 6s 25ms/step - loss: 0.4692 - accuracy: 0.8296 - val_loss: 0.5757 - val_accuracy: 0.7874\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 6s 25ms/step - loss: 0.4213 - accuracy: 0.8420 - val_loss: 0.5895 - val_accuracy: 0.7815\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 6s 25ms/step - loss: 0.3721 - accuracy: 0.8627 - val_loss: 0.5067 - val_accuracy: 0.8131\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 6s 25ms/step - loss: 0.3309 - accuracy: 0.8753 - val_loss: 0.5984 - val_accuracy: 0.7874\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 6s 25ms/step - loss: 0.3007 - accuracy: 0.8881 - val_loss: 0.5684 - val_accuracy: 0.7944\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 6s 25ms/step - loss: 0.2719 - accuracy: 0.8994 - val_loss: 0.5143 - val_accuracy: 0.8166\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 6s 25ms/step - loss: 0.2157 - accuracy: 0.9169 - val_loss: 0.5498 - val_accuracy: 0.8096\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 6s 25ms/step - loss: 0.2014 - accuracy: 0.9220 - val_loss: 0.6043 - val_accuracy: 0.8107\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 6s 27ms/step - loss: 0.1703 - accuracy: 0.9383 - val_loss: 0.6053 - val_accuracy: 0.8026\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 7s 27ms/step - loss: 0.1377 - accuracy: 0.9508 - val_loss: 0.6842 - val_accuracy: 0.7991\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 6s 25ms/step - loss: 0.1312 - accuracy: 0.9535 - val_loss: 0.7170 - val_accuracy: 0.8049\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 6s 26ms/step - loss: 0.0912 - accuracy: 0.9696 - val_loss: 0.7905 - val_accuracy: 0.8037\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 6s 26ms/step - loss: 0.0884 - accuracy: 0.9693 - val_loss: 0.8063 - val_accuracy: 0.7956\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 6s 25ms/step - loss: 0.0789 - accuracy: 0.9693 - val_loss: 0.8847 - val_accuracy: 0.7909\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 6s 26ms/step - loss: 0.0659 - accuracy: 0.9776 - val_loss: 0.9551 - val_accuracy: 0.7967\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(tf.convert_to_tensor(training_images_list), tf.convert_to_tensor(training_labels_list), epochs=20, \n",
    "                    validation_data=(tf.convert_to_tensor(validation_images_list), tf.convert_to_tensor(validation_labels_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
